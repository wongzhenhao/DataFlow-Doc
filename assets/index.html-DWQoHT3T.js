import{_ as e,c as d,b as r,o as a}from"./app-GpZKBnvo.js";const s={};function o(i,t){return a(),d("div",null,t[0]||(t[0]=[r('<h2 id="pure-video-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#pure-video-evaluation-metrics"><span>Pure Video Evaluation Metrics</span></a></h2><h3 id="metric-categories" tabindex="-1"><a class="header-anchor" href="#metric-categories"><span>Metric Categories</span></a></h3><table><thead><tr><th>Category Description</th><th>Metric List</th></tr></thead><tbody><tr><td>Based on Video Statistics</td><td>Motion Score</td></tr><tr><td>Based on Pre-trained Models</td><td>FastVQAScorer, FasterVQAScorer, DOVERScorer</td></tr></tbody></table><h3 id="metric-descriptions" tabindex="-1"><a class="header-anchor" href="#metric-descriptions"><span>Metric Descriptions</span></a></h3><table><thead><tr><th>Name</th><th>Evaluation Metric</th><th>Dimension</th><th>Description</th><th>Value Range</th></tr></thead><tbody><tr><td>VideoMotionScorer</td><td>Motion Score</td><td>Statistical</td><td>Calculates the magnitude of optical flow vectors between frames as the score</td><td></td></tr><tr><td><a href="https://arxiv.org/abs/2207.02595v1" target="_blank" rel="noopener noreferrer">FastVQAScorer</a></td><td>Pre-trained Model Scoring</td><td>Model</td><td>Scorer based on Video Swin Transformer, incorporating the Fragment Sampling module, which improves accuracy and speed</td><td>[0,1]</td></tr><tr><td><a href="https://arxiv.org/abs/2210.05357" target="_blank" rel="noopener noreferrer">FasterVQAScorer</a></td><td>Pre-trained Model Scoring</td><td>Model</td><td>An optimized version of FastVQAScorer, with improvements to the Fragment Sampling module, achieving significant speed enhancements</td><td>[0,1]</td></tr><tr><td><a href="https://arxiv.org/abs/2211.04894" target="_blank" rel="noopener noreferrer">DOVERScorer</a></td><td>Pre-trained Model Scoring</td><td>Model</td><td>Based on FastVQAScorer, it provides scores from both technical and aesthetic perspectives</td><td></td></tr></tbody></table><h3 id="reference-values" tabindex="-1"><a class="header-anchor" href="#reference-values"><span>Reference Values</span></a></h3><p>To better provide data quality references, we evaluated the KoNViD-1k dataset using the above metrics, and the distribution of the evaluation values is as follows:</p><table class="tg"><thead><tr><th class="tg-0pky">Metric</th><th class="tg-0pky">Name</th><th class="tg-0pky">Mean</th><th class="tg-0pky">Variance</th><th class="tg-0pky">Max</th><th class="tg-0pky">Min</th></tr></thead><tbody><tr><td class="tg-0pky">Motion Score</td><td class="tg-0pky"></td><td class="tg-0pky">Calculates the magnitude of the optical flow vectors between video frames as the score. The stronger the motion in the video, the greater the frame-to-frame changes, and the higher the score</td><td class="tg-0pky">6.2745</td><td class="tg-0pky">19.28</td><td class="tg-0pky">25.23</td><td class="tg-0pky">0.001623</td></tr><tr><td class="tg-0pky">FastVQA</td><td class="tg-0pky"></td><td class="tg-0pky">The score obtained using the FastVQAScorer module. The better the video quality, the higher the score</td><td class="tg-0pky">0.4987</td><td class="tg-0pky">0.04554</td><td class="tg-0pky">0.9258</td><td class="tg-0pky">0.007619</td></tr><tr><td class="tg-0pky">FasterVQA</td><td class="tg-0pky"></td><td class="tg-0pky">The score obtained using the FasterVQAScorer module. The better the video quality, the higher the score</td><td class="tg-0pky">0.5134</td><td class="tg-0pky">0.04558</td><td class="tg-0pky">0.9066</td><td class="tg-0pky">0.03686</td></tr><tr><td class="tg-0pky" rowspan="2">DOVER</td><td class="tg-0pky">technical</td><td class="tg-0pky">One of the scores obtained using the DOVERScorer module, the better the technical quality of the video, the higher the score</td><td class="tg-0pky">-0.1107</td><td class="tg-0pky">0.001755</td><td class="tg-0pky">-0.006550</td><td class="tg-0pky">-0.3175</td></tr><tr><td class="tg-0pky">aesthetic</td><td class="tg-0pky">One of the scores obtained using the DOVERScorer module, the better the aesthetic quality of the video, the higher the score</td><td class="tg-0pky">-0.008419</td><td class="tg-0pky">0.004569</td><td class="tg-0pky">0.1869</td><td class="tg-0pky">-0.2629</td></tr></tbody></table><h2 id="video-text-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#video-text-evaluation-metrics"><span>Video-Text Evaluation Metrics</span></a></h2><table><thead><tr><th>Category Description</th><th>Metric List</th></tr></thead><tbody><tr><td>Based on Pre-trained Vision-Language Models</td><td>EMScore, PAC-S</td></tr></tbody></table><table><thead><tr><th>Name</th><th>Evaluation Metric</th><th>Dimension</th><th>Description</th><th>Value Range</th></tr></thead><tbody><tr><td><a href="https://arxiv.org/abs/2111.08919" target="_blank" rel="noopener noreferrer">EMScorer</a></td><td>Video-Text Similarity Scoring</td><td>Model</td><td>A video-text scorer based on CLIP, supporting both with-reference and no-reference scoring.</td><td>[0,1]</td></tr><tr><td><a href="https://arxiv.org/abs/2303.12112" target="_blank" rel="noopener noreferrer">PACScorer</a></td><td>Video-Text Similarity Scoring</td><td>Model</td><td>A video-text scorer based on CLIP, with tuned CLIP Encoder on top of EMScore</td><td>[0,1]</td></tr></tbody></table><h3 id="reference-values-1" tabindex="-1"><a class="header-anchor" href="#reference-values-1"><span>Reference Values</span></a></h3><p>To provide better data quality reference, we evaluated the VATEX dataset (<a href="https://huggingface.co/datasets/lmms-lab/VATEX" target="_blank" rel="noopener noreferrer">link</a>) using the above metrics, and the distribution of the metric values is as follows:</p><table><thead><tr><th>Metric</th><th>Name</th><th>Description</th><th>Mean</th><th>Variance</th><th>Max</th><th>Min</th></tr></thead><tbody><tr><td>EMScorer</td><td>figr_F</td><td>The score obtained using the EMScorer module. The higher the similarity between the video and text at a fine-grained level, the higher the score</td><td>0.2712</td><td>0.0003667</td><td>0.3461</td><td>0.1987</td></tr><tr><td></td><td>cogr</td><td>The score obtained using the EMScorer module. The higher the similarity between the video and text at a coarse-grained level, the higher the score</td><td>0.3106</td><td>0.0009184</td><td>0.4144</td><td>0.18</td></tr><tr><td></td><td>full_F</td><td>The score obtained using the EMScorer module, which is the arithmetic mean of the above two scores</td><td>0.2909</td><td>0.0005776</td><td>0.3712</td><td>0.3807</td></tr><tr><td>PACScorer</td><td>figr_F</td><td>The score obtained using the PACScorer module. The higher the similarity between the video and text at a fine-grained level, the higher the score</td><td>0.36553</td><td>0.0004902</td><td>0.4456</td><td>0.2778</td></tr><tr><td></td><td>cogr</td><td>The score obtained using the PACScorer module. The higher the similarity between the video and text at a coarse-grained level, the higher the score</td><td>0.4160</td><td>0.001021</td><td>0.5222</td><td>0.2510</td></tr><tr><td></td><td>full_F</td><td>The score obtained using the PACScorer module, which is the arithmetic mean of the above two scores</td><td>0.3908</td><td>0.0006854</td><td>0.4761</td><td>0.2681</td></tr></tbody></table>',14)]))}const n=e(s,[["render",o]]),c=JSON.parse('{"path":"/en/guide/zqsddulh/","title":"Video Data Evaluation Metrics","lang":"en-US","frontmatter":{"title":"Video Data Evaluation Metrics","createTime":"2025/06/09 11:43:25","permalink":"/en/guide/zqsddulh/"},"readingTime":{"minutes":2.69,"words":806},"git":{"createdTime":1749441278000,"updatedTime":1750128958000,"contributors":[{"name":"Sunnyhaze","username":"Sunnyhaze","email":"mxch1122@126.com","commits":1,"avatar":"https://avatars.githubusercontent.com/Sunnyhaze?v=4","url":"https://github.com/Sunnyhaze"},{"name":"Ma, Xiaochen","username":"","email":"mxch1122@126.com","commits":1,"avatar":"https://gravatar.com/avatar/c86bc98abf428aa442dfc12c76e70e324a551ebc637e5ed6634d60fbd3811221?d=retro"}]},"filePathRelative":"en/notes/guide/metrics/video_metrics.md","headers":[]}');export{n as comp,c as data};
