import{_ as e,c as d,b as a,o as s}from"./app-GpZKBnvo.js";const r={};function l(c,t){return s(),d("div",null,t[0]||(t[0]=[a('<h2 id="pure-image-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#pure-image-evaluation-metrics"><span>Pure Image Evaluation Metrics</span></a></h2><h3 id="metric-classification" tabindex="-1"><a class="header-anchor" href="#metric-classification"><span>Metric Classification</span></a></h3><table><thead><tr><th>Category Description</th><th>Metric List</th></tr></thead><tbody><tr><td>Based on Image Statistics</td><td>BRISQUE, ILNIQE, NIQE, PIQE, FID, KID, IS</td></tr><tr><td>Based on Neural Networks</td><td>ARNIQA, TOPIQ, TReS, MANIQA, MUSIQ, DBCNN, PaQ-2-PiQ, HyperIQA, NIMA, WaDIQaM, CNNIQA</td></tr><tr><td>Based on Pre-trained Image-Text Models</td><td>Q-Align, CLIPIQA(+), LIQE</td></tr></tbody></table><h3 id="evaluation-metrics-for-real-images" tabindex="-1"><a class="header-anchor" href="#evaluation-metrics-for-real-images"><span>Evaluation Metrics for Real Images</span></a></h3><h4 id="metric-introduction" tabindex="-1"><a class="header-anchor" href="#metric-introduction"><span>Metric Introduction</span></a></h4><p>This repository uses non-reference (NR) algorithms from the <a href="https://github.com/chaofengc/IQA-PyTorch" target="_blank" rel="noopener noreferrer">pyiqa</a> package for pure image data quality assessment. Introductions to each evaluation metric can be found in the <a href="https://github.com/chaofengc/IQA-PyTorch/blob/main/docs/ModelCard.md" target="_blank" rel="noopener noreferrer">Py-IQA Model Card</a>.</p><p>Note: When the same metric uses different training datasets, we distinguish them using <code>Metric Name-Dataset Name</code>. For example, <code>arniqa-csiq</code> uses <code>csiq</code> as the dataset name. When the dataset name is not specified, it defaults to <code>koniq</code>, such as <code>arniqa</code> which corresponds to the <code>koniq</code> dataset.</p><table><thead><tr><th>Metric</th><th>Name (for <code>datagym.get_scorer()</code>)</th><th>Evaluation Dimension</th><th>Introduction</th><th>Value Range</th><th>Official Repository or Paper</th></tr></thead><tbody><tr><td>Q-Align</td><td><code>qalign</code> (with quality[default], aesthetic options)</td><td>Based on Pre-trained Image-Text Model</td><td>Scoring using Visual LLM. The larger the value, the higher the quality.</td><td></td><td>[1,5]</td></tr><tr><td>LIQE</td><td><code>liqe</code>, <code>liqe_mix</code></td><td>Based on Pre-trained Image-Text Model</td><td>Based on CLIP. The larger the value, the higher the quality.</td><td>[1,5]</td><td><a href="https://github.com/zwx8981/LIQE" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>ARNIQA</td><td><code>arniqa</code>, <code>arniqa-live</code>, <code>arniqa-csiq</code>, <code>arniqa-tid</code>, <code>arniqa-kadid</code>, <code>arniqa-clive</code>, <code>arniqa-flive</code>, <code>arniqa-spaq</code></td><td>Based on Neural Networks</td><td>Learning the Image Distortion Manifold. The larger the value, the higher the quality.</td><td></td><td><a href="https://arxiv.org/abs/2310.14918" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>TOPIQ</td><td><code>topiq_nr</code>, <code>topiq_nr-flive</code>, <code>topiq_nr-spaq</code></td><td>Based on Neural Networks</td><td>Semantic-based Top-down Image Quality Assessment. The larger the value, the higher the quality.</td><td>[0,1]</td><td><a href="https://arxiv.org/abs/2308.03060" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>TReS</td><td><code>tres</code>, <code>tres-flive</code></td><td>Based on Neural Networks</td><td>Enhancing Metric Robustness through Relative Ranking and Self-consistency. The larger the value, the higher the quality.</td><td>[0,100]</td><td><a href="https://github.com/isalirezag/TReS" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>CLIPIQA(+)</td><td><code>clipiqa</code>, <code>clipiqa+</code>, <code>clipiqa+_vitL14_512</code>, <code>clipiqa+_rn50_512</code></td><td>Based on Pre-trained Image-Text Model</td><td>Based on CLIP with Antonym prompt pairing. CLIPIQA(+) with different backbone networks, default is RN50. The larger the value, the higher the quality.</td><td>[0,1]</td><td><a href="https://github.com/IceClear/CLIP-IQA" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>MANIQA</td><td><code>maniqa</code>, <code>maniqa-kadid</code>, <code>maniqa-pipal</code></td><td>Based on Neural Networks</td><td>Designed a Multi-dimension Attention Network for Quality Assessment. The larger the value, the higher the quality.</td><td></td><td><a href="https://arxiv.org/abs/2204.08958" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>MUSIQ</td><td><code>musiq</code>, <code>musiq-spaq</code>, <code>musiq-paq2piq</code>, <code>musiq-ava</code></td><td>Based on Neural Networks</td><td>Designed a Multi-scale Image Quality Assessment Transformer. The larger the value, the higher the quality.</td><td></td><td><a href="https://arxiv.org/abs/2108.05997" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>DBCNN</td><td><code>dbcnn</code></td><td>Based on Neural Networks</td><td>Designed a Bilinear Model to Address Synthetic and Realistic Distortions. The larger the value, the higher the quality.</td><td></td><td><a href="https://ieeexplore.ieee.org/document/8576582" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>PaQ-2-PiQ</td><td><code>paq2piq</code></td><td>Based on Neural Networks</td><td>Designed a Quality Assessment Structure that Generates Global-to-Local and Local-to-Global Inferences. The larger the value, the higher the quality.</td><td></td><td><a href="https://baidut.github.io/PaQ-2-PiQ/" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>HyperIQA</td><td><code>hyperiqa</code></td><td>Based on Neural Networks</td><td>Designed an Adaptive Hypernetwork Architecture to Handle Realistic Image Distortions. The larger the value, the higher the quality.</td><td></td><td><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>NIMA</td><td><code>nima</code>, <code>nima-vgg16-ava</code></td><td>Based on Neural Networks</td><td>Predicting Human Opinion Scores using Convolutional Neural Networks <strong>Distribution</strong>. The larger the value, the higher the quality.</td><td></td><td><a href="https://arxiv.org/abs/1709.05424" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>WaDIQaM</td><td><code>wadiqam_nr</code></td><td>Based on Neural Networks</td><td>Based on Convolutional Neural Networks. The larger the value, the higher the quality.</td><td></td><td><a href="https://ieeexplore.ieee.org/abstract/document/8063957" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>CNNIQA</td><td><code>cnniqa</code></td><td>Based on Neural Networks</td><td>Based on Convolutional Neural Networks. The larger the value, the higher the quality.</td><td></td><td><a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Convolutional_Neural_Networks_2014_CVPR_paper.pdf" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>NRQM(Ma)<sup>2</sup></td><td><code>nrqm</code></td><td>Super-Resolution Image Assessment</td><td>Based on Image Statistics. The smaller the value, the higher the quality.</td><td></td><td><a href="https://arxiv.org/abs/1612.05890" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>PI (Perceptual Index)</td><td><code>pi</code></td><td>Super-Resolution Image Assessment</td><td>Based on Ma&#39;s score and NIQE. The smaller the value, the higher the quality.</td><td></td><td><a href="https://arxiv.org/abs/1711.06077" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>BRISQUE</td><td><code>brisque</code>, <code>brisque_matlab</code></td><td>Based on Image Statistics</td><td>Assessed in the Spatial Domain; Low Computational Complexity. The smaller the value, the higher the quality.</td><td></td><td><a href="https://ieeexplore.ieee.org/document/6272356" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>ILNIQE</td><td><code>ilniqe</code></td><td>Based on Image Statistics</td><td>Based on Natural Image Statistical Features. The smaller the value, the higher the quality.</td><td></td><td><a href="https://ieeexplore.ieee.org/document/7094273" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>NIQE</td><td><code>niqe</code>, <code>niqe_matlab</code></td><td>Based on Image Statistics</td><td>Based on Statistical Features of Natural, Undistorted Image Data. The smaller the value, the higher the quality.</td><td></td><td><a href="https://ieeexplore.ieee.org/document/6353522" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>PIQE</td><td><code>piqe</code></td><td>Based on Image Statistics</td><td>Extract Local Features to Predict Quality; Low Computational Complexity. The smaller the value, the higher the quality.</td><td></td><td><a href="https://ieeexplore.ieee.org/document/7084843" target="_blank" rel="noopener noreferrer">paper</a></td></tr></tbody></table><h4 id="reference-values" tabindex="-1"><a class="header-anchor" href="#reference-values"><span>Reference Values</span></a></h4><p>To better provide data quality references, we have evaluated the MSCOCO 2017 train using the above metrics, and the distribution of metric values obtained is as follows:</p><table class="tg"><thead><tr><th class="tg-0pky">Metric</th><th class="tg-0pky">Name</th><th class="tg-0pky">Mean</th><th class="tg-0pky">Variance</th><th class="tg-0pky">Maximum</th><th class="tg-0pky">Minimum</th></tr></thead><tbody><tr><td class="tg-0pky">Q-Align</td><td class="tg-0pky">qalign</td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td></tr><tr><td class="tg-0pky" rowspan="2">LIQE</td><td class="tg-0pky">liqe</td><td class="tg-0pky">4.152</td><td class="tg-0pky">1.004</td><td class="tg-0pky">5.000</td><td class="tg-0pky">1.000</td></tr><tr><td class="tg-0pky">liqe_mix</td><td class="tg-0pky">4.090</td><td class="tg-0pky">0.893</td><td class="tg-0pky">5.000</td><td class="tg-0pky">1.000</td></tr><tr><td class="tg-0pky" rowspan="9">ARNIQA</td><td class="tg-0pky">arniqa</td><td class="tg-0pky">0.705</td><td class="tg-0pky">0.069</td><td class="tg-0pky">0.867</td><td class="tg-0pky">0.150</td></tr><tr><td class="tg-0pky">arniqa-clive</td><td class="tg-0pky">0.649</td><td class="tg-0pky">0.103</td><td class="tg-0pky">0.961</td><td class="tg-0pky">-0.105</td></tr><tr><td class="tg-0pky">arniqa-csiq</td><td class="tg-0pky">0.900</td><td class="tg-0pky">0.073</td><td class="tg-0pky">1.081</td><td class="tg-0pky">0.319</td></tr><tr><td class="tg-0pky">arniqa-flive</td><td class="tg-0pky">0.724</td><td class="tg-0pky">0.036</td><td class="tg-0pky">0.838</td><td class="tg-0pky">0.097</td></tr><tr><td class="tg-0pky">arniqa-kadid</td><td class="tg-0pky">0.635</td><td class="tg-0pky">0.122</td><td class="tg-0pky">0.965</td><td class="tg-0pky">-0.013</td></tr><tr><td class="tg-0pky">arniqa-koniq</td><td class="tg-0pky">0.705</td><td class="tg-0pky">0.069</td><td class="tg-0pky">0.867</td><td class="tg-0pky">0.150</td></tr><tr><td class="tg-0pky">arniqa-live</td><td class="tg-0pky">0.788</td><td class="tg-0pky">0.069</td><td class="tg-0pky">0.958</td><td class="tg-0pky">0.227</td></tr><tr><td class="tg-0pky">arniqa-spqa</td><td class="tg-0pky">0.699</td><td class="tg-0pky">0.104</td><td class="tg-0pky">1.100</td><td class="tg-0pky">0.056</td></tr><tr><td class="tg-0pky">arniqa-tid</td><td class="tg-0pky">0.548</td><td class="tg-0pky">0.081</td><td class="tg-0pky">0.803</td><td class="tg-0pky">0.140</td></tr><tr><td class="tg-0pky" rowspan="5">TOPIQ</td><td class="tg-0pky">topiq_nr</td><td class="tg-0pky">0.610</td><td class="tg-0pky">0.116</td><td class="tg-0pky">0.851</td><td class="tg-0pky">0.073</td></tr><tr><td class="tg-0pky">topiq_iaa_res50</td><td class="tg-0pky">5.013</td><td class="tg-0pky">0.492</td><td class="tg-0pky">6.969</td><td class="tg-0pky">2.812</td></tr><tr><td class="tg-0pky">topiq_iaa</td><td class="tg-0pky">4.838</td><td class="tg-0pky">0.539</td><td class="tg-0pky">7.129</td><td class="tg-0pky">2.607</td></tr><tr><td class="tg-0pky">topiq_nr-flive</td><td class="tg-0pky">0.728</td><td class="tg-0pky">0.036</td><td class="tg-0pky">0.825</td><td class="tg-0pky">0.371</td></tr><tr><td class="tg-0pky">topiq_nr-spaq</td><td class="tg-0pky">0.679</td><td class="tg-0pky">0.102</td><td class="tg-0pky">0.930</td><td class="tg-0pky">0.119</td></tr><tr><td class="tg-0pky" rowspan="4">CLIPIQA(+)</td><td class="tg-0pky">clipiqa</td><td class="tg-0pky">0.622</td><td class="tg-0pky">0.149</td><td class="tg-0pky">0.934</td><td class="tg-0pky">0.056</td></tr><tr><td class="tg-0pky">clipiqa+</td><td class="tg-0pky">0.659</td><td class="tg-0pky">0.100</td><td class="tg-0pky">0.918</td><td class="tg-0pky">0.130</td></tr><tr><td class="tg-0pky">clipiqa+_rn50_512</td><td class="tg-0pky">0.571</td><td class="tg-0pky">0.122</td><td class="tg-0pky">0.883</td><td class="tg-0pky">0.050</td></tr><tr><td class="tg-0pky">clipiqa+_vitL14_512</td><td class="tg-0pky">0.593</td><td class="tg-0pky">0.128</td><td class="tg-0pky">0.893</td><td class="tg-0pky">0.077</td></tr><tr><td class="tg-0pky" rowspan="4">MANIQA</td><td class="tg-0pky">maniqa</td><td class="tg-0pky">0.454</td><td class="tg-0pky">0.106</td><td class="tg-0pky">0.789</td><td class="tg-0pky">0.021</td></tr><tr><td class="tg-0pky">maniqa-kadid</td><td class="tg-0pky">0.637</td><td class="tg-0pky">0.122</td><td class="tg-0pky">0.877</td><td class="tg-0pky">0.075</td></tr><tr><td class="tg-0pky">maniqa-koniq</td><td class="tg-0pky">0.454</td><td class="tg-0pky">0.106</td><td class="tg-0pky">0.789</td><td class="tg-0pky">0.021</td></tr><tr><td class="tg-0pky">maniqa-pipal</td><td class="tg-0pky">0.676</td><td class="tg-0pky">0.062</td><td class="tg-0pky">0.888</td><td class="tg-0pky">0.228</td></tr><tr><td class="tg-0pky" rowspan="5">MUSIQ</td><td class="tg-0pky">musiq</td><td class="tg-0pky">69.086</td><td class="tg-0pky">7.833</td><td class="tg-0pky">79.559</td><td class="tg-0pky">12.679</td></tr><tr><td class="tg-0pky">musiq-ava</td><td class="tg-0pky">4.939</td><td class="tg-0pky">0.546</td><td class="tg-0pky">7.269</td><td class="tg-0pky">2.434</td></tr><tr><td class="tg-0pky">musiq-koniq</td><td class="tg-0pky">69.086</td><td class="tg-0pky">7.833</td><td class="tg-0pky">79.559</td><td class="tg-0pky">12.679</td></tr><tr><td class="tg-0pky">musiq-paq2piq</td><td class="tg-0pky">72.792</td><td class="tg-0pky">3.520</td><td class="tg-0pky">79.772</td><td class="tg-0pky">39.551</td></tr><tr><td class="tg-0pky">musiq-spaq</td><td class="tg-0pky">70.534</td><td class="tg-0pky">8.661</td><td class="tg-0pky">81.385</td><td class="tg-0pky">14.290</td></tr><tr><td class="tg-0pky">DBCNN</td><td class="tg-0pky">dbcnn</td><td class="tg-0pky">0.634</td><td class="tg-0pky">0.100</td><td class="tg-0pky">0.834</td><td class="tg-0pky">0.143</td></tr><tr><td class="tg-0pky">PaQ-2-PiQ</td><td class="tg-0pky">paq2piq</td><td class="tg-0pky">74.669</td><td class="tg-0pky">3.731</td><td class="tg-0pky">85.906</td><td class="tg-0pky">15.859</td></tr><tr><td class="tg-0pky">HyperIQA</td><td class="tg-0pky">hyperiqa</td><td class="tg-0pky">0.618</td><td class="tg-0pky">0.105</td><td class="tg-0pky">0.843</td><td class="tg-0pky">0.082</td></tr><tr><td class="tg-0pky" rowspan="4">NIMA</td><td class="tg-0pky">nima</td><td class="tg-0pky">4.941</td><td class="tg-0pky">0.537</td><td class="tg-0pky">7.056</td><td class="tg-0pky">2.463</td></tr><tr><td class="tg-0pky">nima-koniq</td><td class="tg-0pky">0.654</td><td class="tg-0pky">0.084</td><td class="tg-0pky">0.849</td><td class="tg-0pky">0.048</td></tr><tr><td class="tg-0pky">nima-spaq</td><td class="tg-0pky">71.036</td><td class="tg-0pky">10.099</td><td class="tg-0pky">98.191</td><td class="tg-0pky">12.237</td></tr><tr><td class="tg-0pky">nima-vgg-ava</td><td class="tg-0pky">5.040</td><td class="tg-0pky">0.503</td><td class="tg-0pky">7.327</td><td class="tg-0pky">2.374</td></tr><tr><td class="tg-0pky">WaDIQaM</td><td class="tg-0pky">wadiqam_nr</td><td class="tg-0pky">-0.066</td><td class="tg-0pky">0.207</td><td class="tg-0pky">0.377</td><td class="tg-0pky">-1.281</td></tr><tr><td class="tg-0pky">CNNIQA</td><td class="tg-0pky">cnniqa</td><td class="tg-0pky">0.655</td><td class="tg-0pky">0.070</td><td class="tg-0pky">0.759</td><td class="tg-0pky">0.089</td></tr><tr><td class="tg-0pky">NRQM(Ma)^2^</td><td class="tg-0pky">nrqm</td><td class="tg-0pky">8.050</td><td class="tg-0pky">1.001</td><td class="tg-0pky">9.222</td><td class="tg-0pky">1.600</td></tr><tr><td class="tg-0pky">PI</td><td class="tg-0pky">pi</td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td></tr><tr><td class="tg-0pky">BRISQUE</td><td class="tg-0pky">brisque</td><td class="tg-0pky">13.777</td><td class="tg-0pky">11.891</td><td class="tg-0pky">184.089</td><td class="tg-0pky">-67.742</td></tr><tr><td class="tg-0pky">ILNIQE</td><td class="tg-0pky">ilniqe</td><td class="tg-0pky">22.919</td><td class="tg-0pky">6.589</td><td class="tg-0pky">154.256</td><td class="tg-0pky">12.733</td></tr><tr><td class="tg-0pky">NIQE</td><td class="tg-0pky">niqe</td><td class="tg-0pky">3.718</td><td class="tg-0pky">1.082</td><td class="tg-0pky">55.155</td><td class="tg-0pky">1.430</td></tr><tr><td class="tg-0pky">PIQE</td><td class="tg-0pky">piqe</td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td></tr></tbody></table><h3 id="evaluation-metrics-for-generated-images" tabindex="-1"><a class="header-anchor" href="#evaluation-metrics-for-generated-images"><span>Evaluation Metrics for Generated Images</span></a></h3><table><thead><tr><th>Metric</th><th>Name for <code>datagym.get_scorer()</code></th><th>Evaluation Dimension</th><th>Description</th><th>Range</th><th>Official Repository or Paper</th></tr></thead><tbody><tr><td>FID</td><td><code>fid_score</code></td><td>Statistical difference between generated and real images</td><td>Uses Inception network to calculate features and then the statistical distance between two datasets to evaluate the quality of generative models.</td><td>Best value is 0, lower values indicate smaller differences and higher image quality, no upper limit</td><td><a href="https://arxiv.org/pdf/1706.08500" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>KID</td><td><code>kid_score</code></td><td>Unbiased quality estimation of generated images</td><td>Kernel Inception Distance, uses Inception network features to calculate MMD, providing an unbiased estimation of the quality of generated images.</td><td>Best value is 0, lower values indicate lower bias and better image quality, no upper limit</td><td><a href="https://arxiv.org/abs/1801.01401" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>IS</td><td><code>is_score</code></td><td>Diversity and clarity of generated images</td><td>Evaluates the diversity and clarity of images by calculating the entropy of the Inception network&#39;s output.</td><td>Higher values indicate better image quality, typically scores range from 1 to 10, but no specific upper limit</td><td><a href="https://arxiv.org/pdf/1606.03498" target="_blank" rel="noopener noreferrer">paper</a></td></tr></tbody></table><h3 id="image-text-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#image-text-evaluation-metrics"><span>Image-Text Evaluation Metrics</span></a></h3><h4 id="image-text-alignment-metrics" tabindex="-1"><a class="header-anchor" href="#image-text-alignment-metrics"><span>Image-Text Alignment Metrics</span></a></h4><p>Higher metric values indicate better alignment between images and captions.</p><table><thead><tr><th>Metric</th><th>Name for <code>datagym.get_scorer()</code></th><th>Data Type</th><th>Description</th><th>Range</th><th>Official Repository or Paper</th></tr></thead><tbody><tr><td>CLIP</td><td><code>clip</code></td><td>image-caption</td><td>Classic image-text alignment score. The larger the value, the higher the alignment degree.</td><td>[0,1]</td><td><a href="https://github.com/openai/CLIP" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>LongCLIP</td><td><code>longclip</code></td><td>image-caption</td><td>CLIP with longer text input and finer granularity. The larger the value, the higher the alignment degree.</td><td>[0,1]</td><td><a href="https://github.com/beichenzbc/Long-CLIP" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>FLEUR</td><td><code>fleur</code></td><td>image-caption</td><td>Scores using the LLaVA model. The larger the value, the higher the alignment degree.</td><td>[0,1]</td><td><a href="https://github.com/Yebin46/FLEUR" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>VQA Score</td><td><code>vqa_score</code></td><td>image-caption</td><td>Scores using the CLIP-FlanT5 model. The larger the value, the higher the alignment degree.</td><td>[0,1]</td><td><a href="https://github.com/linzhiqiu/t2v_metrics" target="_blank" rel="noopener noreferrer">code</a></td></tr></tbody></table><h4 id="sft-data-evaluation-metrics-for-image-text" tabindex="-1"><a class="header-anchor" href="#sft-data-evaluation-metrics-for-image-text"><span>SFT Data Evaluation Metrics for Image-Text</span></a></h4><table><thead><tr><th>Metric Name</th><th>Evaluation Dimension</th><th>Data Type</th><th>Description</th><th>Range</th><th>Official Repository or Paper</th></tr></thead><tbody><tr><td>visual_dialog_score</td><td>Image-dialog alignment</td><td>image-dialog</td><td>Use the LLaVA model to judge the correctness of the dialogue. The larger the value, the higher the alignment degree.</td><td>(-âˆž,0]</td><td>/</td></tr></tbody></table>',19)]))}const o=e(r,[["render",l]]),n=JSON.parse('{"path":"/en/guide/lcyxng2w/","title":"Image Data Evaluation Metrics","lang":"en-US","frontmatter":{"title":"Image Data Evaluation Metrics","createTime":"2025/06/09 11:43:25","permalink":"/en/guide/lcyxng2w/"},"readingTime":{"minutes":9.17,"words":2752},"git":{"createdTime":1749441278000,"updatedTime":1750128958000,"contributors":[{"name":"Sunnyhaze","username":"Sunnyhaze","email":"mxch1122@126.com","commits":1,"avatar":"https://avatars.githubusercontent.com/Sunnyhaze?v=4","url":"https://github.com/Sunnyhaze"},{"name":"Ma, Xiaochen","username":"","email":"mxch1122@126.com","commits":1,"avatar":"https://gravatar.com/avatar/c86bc98abf428aa442dfc12c76e70e324a551ebc637e5ed6634d60fbd3811221?d=retro"}]},"filePathRelative":"en/notes/guide/metrics/image_metrics.md","headers":[]}');export{o as comp,n as data};
