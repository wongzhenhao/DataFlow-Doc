import{_ as d,c as a,b as s,o as e}from"./app-GpZKBnvo.js";const r={};function c(p,t){return e(),a("div",null,t[0]||(t[0]=[s('<h2 id="纯图像评估指标" tabindex="-1"><a class="header-anchor" href="#纯图像评估指标"><span>纯图像评估指标</span></a></h2><h3 id="指标分类" tabindex="-1"><a class="header-anchor" href="#指标分类"><span>指标分类</span></a></h3><table><thead><tr><th>类别描述</th><th>指标列表</th></tr></thead><tbody><tr><td>基于图像统计信息</td><td>BRISQUE、ILNIQE、NIQE、PIQE、FID、KID、IS</td></tr><tr><td>基于神经网络</td><td>ARNIQA、TOPIQ、TReS、MANIQA、MUSIQ、DBCNN、PaQ-2-PiQ、HyperIQA、NIMA、WaDIQaM、CNNIQA</td></tr><tr><td>基于预训练图像-文本模型</td><td>Q-Align、CLIPIQA(+)、 LIQE</td></tr></tbody></table><h3 id="针对真实图像的评估指标" tabindex="-1"><a class="header-anchor" href="#针对真实图像的评估指标"><span>针对真实图像的评估指标</span></a></h3><h4 id="指标介绍" tabindex="-1"><a class="header-anchor" href="#指标介绍"><span>指标介绍</span></a></h4><p>本仓库调用<a href="https://github.com/chaofengc/IQA-PyTorch" target="_blank" rel="noopener noreferrer">pyiqa</a>包中的non-reference（NR）算法进行纯图像数据质量评估，各评估指标的介绍可参考<a href="https://github.com/chaofengc/IQA-PyTorch/blob/main/docs/ModelCard.md" target="_blank" rel="noopener noreferrer">Py-IQA Model Card</a>。</p><p>说明：当同一指标使用了不同训练数据集时，我们使用<code>指标名-数据集名</code>进行区分。比如，<code>arniqa-csiq</code>中的<code>csiq</code>即为数据集名称。当没有标注数据集名时，默认为<code>koniq</code>，比如，<code>arniqa</code>对应的数据集为<code>koniq</code>。</p><table><thead><tr><th>指标</th><th>名称（用于<code>datagym.get_scorer()</code>）</th><th>评估维度</th><th>简介</th><th>取值范围</th><th>官方仓库或论文</th></tr></thead><tbody><tr><td>Q-Align</td><td><code>qalign</code> (with quality[default], aesthetic options)</td><td>基于预训练图像-文本模型</td><td>使用视觉LLM进行打分。得分越高代表图像质量越高。</td><td>[1,5]</td><td><a href="https://github.com/Q-Future/Q-Align" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>LIQE</td><td><code>liqe</code>, <code>liqe_mix</code></td><td>基于预训练图像-文本模型</td><td>基于CLIP。得分越高代表图像质量越高。</td><td>[1,5]</td><td><a href="https://github.com/zwx8981/LIQE" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>ARNIQA</td><td><code>arniqa</code>, <code>arniqa-live</code>, <code>arniqa-csiq</code>, <code>arniqa-tid</code>, <code>arniqa-kadid</code>, <code>arniqa-clive</code>, <code>arniqa-flive</code>, <code>arniqa-spaq</code></td><td>基于神经网络</td><td>学习图像失真流形。得分越高代表图像质量越高。</td><td></td><td><a href="https://arxiv.org/abs/2310.14918" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>TOPIQ</td><td><code>topiq_nr</code>, <code>topiq_nr-flive</code>, <code>topiq_nr-spaq</code></td><td>基于神经网络</td><td>基于语义的自顶向下图像质量评估。得分越高代表图像质量越高。</td><td>[0,1]</td><td><a href="https://arxiv.org/abs/2308.03060" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>TReS</td><td><code>tres</code>, <code>tres-flive</code></td><td>基于神经网络</td><td>通过相对排名和自我一致性增强指标的鲁棒性。得分越高代表图像质量越高。</td><td>[0,100]</td><td><a href="https://github.com/isalirezag/TReS" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>CLIPIQA(+)</td><td><code>clipiqa</code>, <code>clipiqa+</code>, <code>clipiqa+_vitL14_512</code>,<code>clipiqa+_rn50_512</code></td><td>基于预训练图像-文本模型</td><td>基于CLIP设计Antonym prompt pairing（反义提示词对）。使用了不同骨干网络的CLIPIQA(+)，默认为RN50。得分越高代表图像质量越高。</td><td>[0,1]</td><td><a href="https://github.com/IceClear/CLIP-IQA" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>MANIQA</td><td><code>maniqa</code>, <code>maniqa-kadid</code>, <code>maniqa-pipal</code></td><td>基于神经网络</td><td>设计了多维注意力网络用于质量评估。得分越高代表图像质量越高。</td><td></td><td><a href="https://arxiv.org/abs/2204.08958" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>MUSIQ</td><td><code>musiq</code>, <code>musiq-spaq</code>, <code>musiq-paq2piq</code>, <code>musiq-ava</code></td><td>基于神经网络</td><td>设计了多尺度图像质量评估Transformer。得分越高代表图像质量越高。</td><td></td><td><a href="https://arxiv.org/abs/2108.05997" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>DBCNN</td><td><code>dbcnn</code></td><td>基于神经网络</td><td>设计了双线性模型来处理合成失真和真实失真。得分越高代表图像质量越高。</td><td></td><td><a href="https://ieeexplore.ieee.org/document/8576582" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>PaQ-2-PiQ</td><td><code>paq2piq</code></td><td>基于神经网络</td><td>设计了产生全局到局部推断以及局部到全局推断的质量评估结构。得分越高代表图像质量越高。</td><td></td><td><a href="https://baidut.github.io/PaQ-2-PiQ/" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>HyperIQA</td><td><code>hyperiqa</code></td><td>基于神经网络</td><td>设计了自适应超网络架构以处理真实世界的图像失真。得分越高代表图像质量越高。</td><td></td><td><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>NIMA</td><td><code>nima</code>, <code>nima-vgg16-ava</code></td><td>基于神经网络</td><td>使用卷积神经网络预测人类意见得分的<strong>分布</strong>。。得分越高代表图像质量越高。</td><td></td><td><a href="https://arxiv.org/abs/1709.05424" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>WaDIQaM</td><td><code>wadiqam_nr</code></td><td>基于神经网络</td><td>基于卷积神经网络。得分越高代表图像质量越高。</td><td></td><td><a href="https://ieeexplore.ieee.org/abstract/document/8063957" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>CNNIQA</td><td><code>cnniqa</code></td><td>基于神经网络</td><td>基于卷积神经网络。得分越高代表图像质量越高。</td><td></td><td><a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Convolutional_Neural_Networks_2014_CVPR_paper.pdf" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>NRQM(Ma)<sup><a href="#fn2">2</a></sup></td><td><code>nrqm</code></td><td>超分辨率图像评估</td><td>基于图像统计信息</td><td></td><td><a href="https://arxiv.org/abs/1612.05890" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>PI(Perceptual Index)</td><td><code>pi</code></td><td>超分辨率图像评估</td><td>基于Ma&#39;s score和NIQE。得分越低代表图像质量越高。</td><td></td><td><a href="https://arxiv.org/abs/1711.06077" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>BRISQUE</td><td><code>brisque</code>, <code>brisque_matlab</code></td><td>基于图像统计信息</td><td>在空间域中进进行评估；计算复杂度低。得分越低代表图像质量越高。</td><td></td><td><a href="https://ieeexplore.ieee.org/document/6272356" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>ILNIQE</td><td><code>ilniqe</code></td><td>基于图像统计信息</td><td>基于自然图像统计特征。得分越低代表图像质量越高。</td><td></td><td><a href="https://ieeexplore.ieee.org/document/7094273" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>NIQE</td><td><code>niqe</code>, <code>niqe_matlab</code></td><td>基于图像统计信息</td><td>基于自然、未失真的图像数据的统计特征。得分越低代表图像质量越高。</td><td></td><td><a href="https://ieeexplore.ieee.org/document/6353522" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>PIQE</td><td><code>piqe</code></td><td>基于图像统计信息</td><td>提取局部特征来预测质量；计算复杂度低。得分越低代表图像质量越高。</td><td></td><td><a href="https://ieeexplore.ieee.org/document/7084843" target="_blank" rel="noopener noreferrer">paper</a></td></tr></tbody></table><h4 id="参考值" tabindex="-1"><a class="header-anchor" href="#参考值"><span>参考值</span></a></h4><p>为更好的提供数据质量参考，我们使用以上指标对MSCOCO 2017 train进行评估，得到的指标数值分布如下:</p><table class="tg"><thead><tr><th class="tg-0pky">指标</th><th class="tg-0pky">名称</th><th class="tg-0pky">均值</th><th class="tg-0pky">方差</th><th class="tg-0pky">最大值</th><th class="tg-0pky">最小值</th></tr></thead><tbody><tr><td class="tg-0pky">Q-Align</td><td class="tg-0pky">qalign</td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td></tr><tr><td class="tg-0pky" rowspan="2">LIQE</td><td class="tg-0pky">liqe</td><td class="tg-0pky">4.152</td><td class="tg-0pky">1.004</td><td class="tg-0pky">5.000</td><td class="tg-0pky">1.000</td></tr><tr><td class="tg-0pky">liqe_mix</td><td class="tg-0pky">4.090</td><td class="tg-0pky">0.893</td><td class="tg-0pky">5.000</td><td class="tg-0pky">1.000</td></tr><tr><td class="tg-0pky" rowspan="9">ARNIQA</td><td class="tg-0pky">arniqa</td><td class="tg-0pky">0.705</td><td class="tg-0pky">0.069</td><td class="tg-0pky">0.867</td><td class="tg-0pky">0.150</td></tr><tr><td class="tg-0pky">arniqa-clive</td><td class="tg-0pky">0.649</td><td class="tg-0pky">0.103</td><td class="tg-0pky">0.961</td><td class="tg-0pky">-0.105</td></tr><tr><td class="tg-0pky">arniqa-csiq</td><td class="tg-0pky">0.900</td><td class="tg-0pky">0.073</td><td class="tg-0pky">1.081</td><td class="tg-0pky">0.319</td></tr><tr><td class="tg-0pky">arniqa-flive</td><td class="tg-0pky">0.724</td><td class="tg-0pky">0.036</td><td class="tg-0pky">0.838</td><td class="tg-0pky">0.097</td></tr><tr><td class="tg-0pky">arniqa-kadid</td><td class="tg-0pky">0.635</td><td class="tg-0pky">0.122</td><td class="tg-0pky">0.965</td><td class="tg-0pky">-0.013</td></tr><tr><td class="tg-0pky">arniqa-koniq</td><td class="tg-0pky">0.705</td><td class="tg-0pky">0.069</td><td class="tg-0pky">0.867</td><td class="tg-0pky">0.150</td></tr><tr><td class="tg-0pky">arniqa-live</td><td class="tg-0pky">0.788</td><td class="tg-0pky">0.069</td><td class="tg-0pky">0.958</td><td class="tg-0pky">0.227</td></tr><tr><td class="tg-0pky">arniqa-spqa</td><td class="tg-0pky">0.699</td><td class="tg-0pky">0.104</td><td class="tg-0pky">1.100</td><td class="tg-0pky">0.056</td></tr><tr><td class="tg-0pky">arniqa-tid</td><td class="tg-0pky">0.548</td><td class="tg-0pky">0.081</td><td class="tg-0pky">0.803</td><td class="tg-0pky">0.140</td></tr><tr><td class="tg-0pky" rowspan="5">TOPIQ</td><td class="tg-0pky">topiq_nr</td><td class="tg-0pky">0.610</td><td class="tg-0pky">0.116</td><td class="tg-0pky">0.851</td><td class="tg-0pky">0.073</td></tr><tr><td class="tg-0pky">topiq_iaa_res50</td><td class="tg-0pky">5.013</td><td class="tg-0pky">0.492</td><td class="tg-0pky">6.969</td><td class="tg-0pky">2.812</td></tr><tr><td class="tg-0pky">topiq_iaa</td><td class="tg-0pky">4.838</td><td class="tg-0pky">0.539</td><td class="tg-0pky">7.129</td><td class="tg-0pky">2.607</td></tr><tr><td class="tg-0pky">topiq_nr-flive</td><td class="tg-0pky">0.728</td><td class="tg-0pky">0.036</td><td class="tg-0pky">0.825</td><td class="tg-0pky">0.371</td></tr><tr><td class="tg-0pky">topiq_nr-spaq</td><td class="tg-0pky">0.679</td><td class="tg-0pky">0.102</td><td class="tg-0pky">0.930</td><td class="tg-0pky">0.119</td></tr><tr><td class="tg-0pky" rowspan="4">CLIPIQA(+)</td><td class="tg-0pky">clipiqa</td><td class="tg-0pky">0.622</td><td class="tg-0pky">0.149</td><td class="tg-0pky">0.934</td><td class="tg-0pky">0.056</td></tr><tr><td class="tg-0pky">clipiqa+</td><td class="tg-0pky">0.659</td><td class="tg-0pky">0.100</td><td class="tg-0pky">0.918</td><td class="tg-0pky">0.130</td></tr><tr><td class="tg-0pky">clipiqa+_rn50_512</td><td class="tg-0pky">0.571</td><td class="tg-0pky">0.122</td><td class="tg-0pky">0.883</td><td class="tg-0pky">0.050</td></tr><tr><td class="tg-0pky">clipiqa+_vitL14_512</td><td class="tg-0pky">0.593</td><td class="tg-0pky">0.128</td><td class="tg-0pky">0.893</td><td class="tg-0pky">0.077</td></tr><tr><td class="tg-0pky" rowspan="4">MANIQA</td><td class="tg-0pky">maniqa</td><td class="tg-0pky">0.454</td><td class="tg-0pky">0.106</td><td class="tg-0pky">0.789</td><td class="tg-0pky">0.021</td></tr><tr><td class="tg-0pky">maniqa-kadid</td><td class="tg-0pky">0.637</td><td class="tg-0pky">0.122</td><td class="tg-0pky">0.877</td><td class="tg-0pky">0.075</td></tr><tr><td class="tg-0pky">maniqa-koniq</td><td class="tg-0pky">0.454</td><td class="tg-0pky">0.106</td><td class="tg-0pky">0.789</td><td class="tg-0pky">0.021</td></tr><tr><td class="tg-0pky">maniqa-pipal</td><td class="tg-0pky">0.676</td><td class="tg-0pky">0.062</td><td class="tg-0pky">0.888</td><td class="tg-0pky">0.228</td></tr><tr><td class="tg-0pky" rowspan="5">MUSIQ</td><td class="tg-0pky">musiq</td><td class="tg-0pky">69.086</td><td class="tg-0pky">7.833</td><td class="tg-0pky">79.559</td><td class="tg-0pky">12.679</td></tr><tr><td class="tg-0pky">musiq-ava</td><td class="tg-0pky">4.939</td><td class="tg-0pky">0.546</td><td class="tg-0pky">7.269</td><td class="tg-0pky">2.434</td></tr><tr><td class="tg-0pky">musiq-koniq</td><td class="tg-0pky">69.086</td><td class="tg-0pky">7.833</td><td class="tg-0pky">79.559</td><td class="tg-0pky">12.679</td></tr><tr><td class="tg-0pky">musiq-paq2piq</td><td class="tg-0pky">72.792</td><td class="tg-0pky">3.520</td><td class="tg-0pky">79.772</td><td class="tg-0pky">39.551</td></tr><tr><td class="tg-0pky">musiq-spaq</td><td class="tg-0pky">70.534</td><td class="tg-0pky">8.661</td><td class="tg-0pky">81.385</td><td class="tg-0pky">14.290</td></tr><tr><td class="tg-0pky">DBCNN</td><td class="tg-0pky">dbcnn</td><td class="tg-0pky">0.634</td><td class="tg-0pky">0.100</td><td class="tg-0pky">0.834</td><td class="tg-0pky">0.143</td></tr><tr><td class="tg-0pky">PaQ-2-PiQ</td><td class="tg-0pky">paq2piq</td><td class="tg-0pky">74.669</td><td class="tg-0pky">3.731</td><td class="tg-0pky">85.906</td><td class="tg-0pky">15.859</td></tr><tr><td class="tg-0pky">HyperIQA</td><td class="tg-0pky">hyperiqa</td><td class="tg-0pky">0.618</td><td class="tg-0pky">0.105</td><td class="tg-0pky">0.843</td><td class="tg-0pky">0.082</td></tr><tr><td class="tg-0pky" rowspan="4">NIMA</td><td class="tg-0pky">nima</td><td class="tg-0pky">4.941</td><td class="tg-0pky">0.537</td><td class="tg-0pky">7.056</td><td class="tg-0pky">2.463</td></tr><tr><td class="tg-0pky">nima-koniq</td><td class="tg-0pky">0.654</td><td class="tg-0pky">0.084</td><td class="tg-0pky">0.849</td><td class="tg-0pky">0.048</td></tr><tr><td class="tg-0pky">nima-spaq</td><td class="tg-0pky">71.036</td><td class="tg-0pky">10.099</td><td class="tg-0pky">98.191</td><td class="tg-0pky">12.237</td></tr><tr><td class="tg-0pky">nima-vgg-ava</td><td class="tg-0pky">5.040</td><td class="tg-0pky">0.503</td><td class="tg-0pky">7.327</td><td class="tg-0pky">2.374</td></tr><tr><td class="tg-0pky">WaDIQaM</td><td class="tg-0pky">wadiqam_nr</td><td class="tg-0pky">-0.066</td><td class="tg-0pky">0.207</td><td class="tg-0pky">0.377</td><td class="tg-0pky">-1.281</td></tr><tr><td class="tg-0pky">CNNIQA</td><td class="tg-0pky">cnniqa</td><td class="tg-0pky">0.655</td><td class="tg-0pky">0.070</td><td class="tg-0pky">0.759</td><td class="tg-0pky">0.089</td></tr><tr><td class="tg-0pky">NRQM(Ma)^2^</td><td class="tg-0pky">nrqm</td><td class="tg-0pky">8.050</td><td class="tg-0pky">1.001</td><td class="tg-0pky">9.222</td><td class="tg-0pky">1.600</td></tr><tr><td class="tg-0pky">PI</td><td class="tg-0pky">pi</td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td></tr><tr><td class="tg-0pky">BRISQUE</td><td class="tg-0pky">brisque</td><td class="tg-0pky">13.777</td><td class="tg-0pky">11.891</td><td class="tg-0pky">184.089</td><td class="tg-0pky">-67.742</td></tr><tr><td class="tg-0pky">ILNIQE</td><td class="tg-0pky">ilniqe</td><td class="tg-0pky">22.919</td><td class="tg-0pky">6.589</td><td class="tg-0pky">154.256</td><td class="tg-0pky">12.733</td></tr><tr><td class="tg-0pky">NIQE</td><td class="tg-0pky">niqe</td><td class="tg-0pky">3.718</td><td class="tg-0pky">1.082</td><td class="tg-0pky">55.155</td><td class="tg-0pky">1.430</td></tr><tr><td class="tg-0pky">PIQE</td><td class="tg-0pky">piqe</td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td><td class="tg-0pky"></td></tr></tbody></table><h3 id="针对生成图像的评估指标" tabindex="-1"><a class="header-anchor" href="#针对生成图像的评估指标"><span>针对生成图像的评估指标</span></a></h3><table><thead><tr><th>指标</th><th>名称（用于<code>datagym.get_scorer()</code>）</th><th>评估维度</th><th>简介</th><th>取值范围</th><th>官方仓库或论文</th></tr></thead><tbody><tr><td>FID</td><td><code>fid_score</code></td><td>生成图像与真实图像间的统计差异</td><td>使用Inception网络计算特征，进而计算两个数据集的统计距离，评估生成模型的质量。</td><td>最佳值为0，较低的值表明较小的差异和更高的图像质量，无上限</td><td><a href="https://arxiv.org/pdf/1706.08500" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>KID</td><td><code>kid_score</code></td><td>生成图像的无偏质量估计</td><td>Kernel Inception Distance，使用Inception网络特征计算MMD，提供对生成图像质量的无偏估计。</td><td>最佳值为0，较低的值表示更低的偏差和更好的图像质量，无上限</td><td><a href="https://arxiv.org/abs/1801.01401" target="_blank" rel="noopener noreferrer">paper</a></td></tr><tr><td>IS</td><td><code>is_score</code></td><td>生成图像的多样性和清晰度</td><td>通过计算生成图像的Inception网络输出的信息熵，评估图像多样性及清晰度。</td><td>值越高表示图像质量越好，通常分数在1到10之间，但无具体上限</td><td><a href="https://arxiv.org/pdf/1606.03498" target="_blank" rel="noopener noreferrer">paper</a></td></tr></tbody></table><h2 id="图像-文本评估指标" tabindex="-1"><a class="header-anchor" href="#图像-文本评估指标"><span>图像-文本评估指标</span></a></h2><h3 id="图文对齐指标" tabindex="-1"><a class="header-anchor" href="#图文对齐指标"><span>图文对齐指标</span></a></h3><p>指标数值越高，则image-caption对齐程度越好。</p><table><thead><tr><th>指标</th><th>名称（用于<code>datagym.get_scorer()</code></th><th>数据类型</th><th>简介</th><th>取值范围</th><th>官方仓库或论文</th></tr></thead><tbody><tr><td>CLIP</td><td><code>clip</code></td><td>image-caption</td><td>经典的图文对齐分数。数值越大，对齐程度越高。</td><td>[0,1]</td><td><a href="https://github.com/openai/CLIP" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>LongCLIP</td><td><code>longclip</code></td><td>image-caption</td><td>可输入更长文本、粒度更细的CLIP。数值越大，对齐程度越高。</td><td>[0,1]</td><td><a href="https://github.com/beichenzbc/Long-CLIP" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>FLEUR</td><td><code>fleur</code></td><td>image-caption</td><td>使用LLaVA模型进行评分。数值越大，对齐程度越高。</td><td>[0,1]</td><td><a href="https://github.com/Yebin46/FLEUR" target="_blank" rel="noopener noreferrer">code</a></td></tr><tr><td>VQA Score</td><td><code>vqa_score</code></td><td>image-caption</td><td>使用CLIP-FlanT5模型进行打分。数值越大，对齐程度越高。</td><td>[0,1]</td><td><a href="https://github.com/linzhiqiu/t2v_metrics" target="_blank" rel="noopener noreferrer">code</a></td></tr></tbody></table><h3 id="图文sft数据评估指标" tabindex="-1"><a class="header-anchor" href="#图文sft数据评估指标"><span>图文SFT数据评估指标</span></a></h3><table><thead><tr><th>指标名称</th><th>评估维度</th><th>数据类型</th><th>简介</th><th>取值范围</th><th>官方仓库或论文</th></tr></thead><tbody><tr><td>vqa_score</td><td>图片-对话对齐度</td><td>image-dialog</td><td>使用LLaVA模型判断对话正误。数值越大，对齐程度越高。</td><td>(-∞,0]</td><td>/</td></tr></tbody></table>',19)]))}const o=d(r,[["render",c]]),g=JSON.parse('{"path":"/zh/guide/654hng98/","title":"图像数据评估指标","lang":"zh-CN","frontmatter":{"title":"图像数据评估指标","createTime":"2025/06/09 11:43:42","permalink":"/zh/guide/654hng98/"},"readingTime":{"minutes":11.22,"words":3365},"git":{"createdTime":1749441278000,"updatedTime":1750128958000,"contributors":[{"name":"Sunnyhaze","username":"Sunnyhaze","email":"mxch1122@126.com","commits":2,"avatar":"https://avatars.githubusercontent.com/Sunnyhaze?v=4","url":"https://github.com/Sunnyhaze"},{"name":"Ma, Xiaochen","username":"","email":"mxch1122@126.com","commits":1,"avatar":"https://gravatar.com/avatar/c86bc98abf428aa442dfc12c76e70e324a551ebc637e5ed6634d60fbd3811221?d=retro"}]},"filePathRelative":"zh/notes/guide/metrics/image_metrics.md","headers":[]}');export{o as comp,g as data};
